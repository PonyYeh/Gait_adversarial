{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# speed up the loading of the training data\n",
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "import torch as th\n",
    "import itertools\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.init as init\n",
    "from onlineTripletloss import *\n",
    "from selector import *\n",
    "from model_SAGAN1_1 import NetG, NetD, NetA\n",
    "from dataset2Loader_newtriplet import CASIABDataset\n",
    "import torch.optim as optim\n",
    "import visdom\n",
    "from torchvision.utils import make_grid\n",
    "\n",
    "Data_Dir = '../GaitRecognition/GEI_CASIA_B/gei/'\n",
    "Model_Name = 'Model_64x64_TripletSAGAN_90_trial8'\n",
    "Model_dir = './Transform_Model/'+ Model_Name\n",
    "if not os.path.isdir(Model_dir):\n",
    "    os.mkdir(Model_dir)\n",
    "    \n",
    "    \n",
    "#python -m visdom.server\n",
    "vis = visdom.Visdom(port=8097)\n",
    "win = None\n",
    "win1 = None\n",
    "netg = NetG(nc=1)\n",
    "netd = NetD(nc=1)\n",
    "neta = NetA(nc=1)\n",
    "device = th.device(\"cuda:1\")\n",
    "\n",
    "# weights init\n",
    "all_mods = itertools.chain()\n",
    "all_mods = itertools.chain(all_mods, [\n",
    "    list(netg.children())[0].children(),\n",
    "    list(netd.children())[0].children(),\n",
    "    list(neta.children())[0].children()\n",
    "])\n",
    "for mod in all_mods:\n",
    "    if isinstance(mod, nn.Conv2d) or isinstance(mod, nn.ConvTranspose2d):\n",
    "#         init.xavier_normal_(tensor, gain=1.)\n",
    "        init.normal_(mod.weight, 0.0, 0.02)\n",
    "    elif isinstance(mod, nn.BatchNorm2d):\n",
    "        init.normal_(mod.weight, 1.0, 0.02)\n",
    "        init.constant_(mod.bias, 0.0)\n",
    "        \n",
    "epoches = 700\n",
    "glr = 0.00001\n",
    "dlr = 0.00004\n",
    "# lr = 0.00002\n",
    "real_label = 1\n",
    "fake_label = 0\n",
    "batchSize = 32\n",
    "n_critic = 0\n",
    "target = '090'\n",
    "lambda_gp = 0\n",
    "beta1 = 0\n",
    "beta2 = 0.9\n",
    "margin = 10\n",
    "g_k = 2\n",
    "\n",
    "netg = netg.to(device)\n",
    "netd = netd.to(device)\n",
    "neta = neta.to(device)\n",
    "netg.train()\n",
    "netd.train()\n",
    "neta.train()\n",
    "dataset = CASIABDataset(data_dir=Data_Dir,target=target)\n",
    "train_loader = th.utils.data.DataLoader(dataset, batch_size=batchSize, shuffle=True, num_workers=2, pin_memory=False)\n",
    "\n",
    "\n",
    "optimG = optim.Adam(netg.parameters(), lr=glr, betas=(beta1, beta2))\n",
    "optimD = optim.Adam(netd.parameters(), lr=dlr, betas=(beta1, beta2))\n",
    "optimA = optim.Adam(neta.parameters(), lr=dlr, betas=(beta1, beta2))\n",
    "# optimG = optim.RMSprop(netg.parameters(), lr=lr)\n",
    "# optimD = optim.RMSprop(netd.parameters(), lr=lr)\n",
    "# optimA = optim.RMSprop(neta.parameters(), lr=lr)\n",
    "\n",
    "print(\"write parameter log...\")\n",
    "with open(Model_dir+\"/snapshot_log.txt\", \"a\") as myfile:\n",
    "            myfile.write('Epoch = {}, margin = {}, dlr = {}, glr={}, g_k={}, batchsize = {}, beta1={}, beta2={}, n_critic = {}, target={},lambda_gp={} \\n'.format(\n",
    "            epoches, margin, dlr, glr, g_k, batchSize, beta1, beta2, n_critic, target, lambda_gp))\n",
    "\n",
    "low_loss = 10\n",
    "\n",
    "def compute_gradient_penalty(D, real_samples, fake_samples):\n",
    "    \"\"\"Calculates the gradient penalty loss for WGAN GP\"\"\"\n",
    "    # Random weight term for interpolation between real and fake samples\n",
    "    alpha = th.rand((batchSize, 1, 1, 1)).to(device).to(th.float32)\n",
    "    # Get random interpolation between real and fake samples\n",
    "    interpolates = (alpha * real_samples + ((1 - alpha) * fake_samples)).requires_grad_(True)\n",
    "\n",
    "    d_interpolates = D(interpolates)\n",
    "    gradients = grad(outputs=d_interpolates, \n",
    "                     inputs=interpolates, \n",
    "                     grad_outputs=th.ones([real_samples.shape[0],1]).to(device).requires_grad_(False),\n",
    "#                      grad_outputs = fake,\n",
    "                     create_graph=True, \n",
    "                     retain_graph=True, \n",
    "                     only_inputs=True)[0]\n",
    "\n",
    "    gradients = gradients.view(gradients.size(0), -1)\n",
    "    gradient_penalty = ((gradients.norm(2, dim=1) - 1) ** 2).mean()\n",
    "    return gradient_penalty\n",
    "\n",
    "print('Training starts')\n",
    "for epoch in range(1,epoches+1):\n",
    "#     for i, (ass_label, noass_label, img) in enumerate(train_loader):\n",
    "    for i, (ass_label, noass_label, noass_img, img, ass_img) in enumerate(train_loader):\n",
    "#        \n",
    "        ass_label = ass_label.to(device).to(th.float32)\n",
    "        noass_label = noass_label.to(device).to(th.float32)\n",
    "        noass_img = noass_img.to(device).to(th.float32)\n",
    "        img = img.to(device).to(th.float32)\n",
    "        ass_img = ass_img.to(device).to(th.float32)\n",
    "\n",
    "        if i % g_k ==0:\n",
    "            # update D\n",
    "            lossD = 0\n",
    "            lossD_ = 0\n",
    "            optimD.zero_grad()\n",
    "            d_out_assreal,dr1 = netd(ass_label)\n",
    "            d_loss_assreal = nn.ReLU()(1.0 - d_out_assreal).mean()\n",
    "\n",
    "            lossD_ += d_loss_assreal\n",
    "            lossD += d_loss_assreal.item()\n",
    "\n",
    "            d_out_noassreal,dr2 = netd(noass_label)\n",
    "            d_loss_noassreal = nn.ReLU()(1.0 - d_out_noassreal).mean()\n",
    "\n",
    "            lossD_ += d_loss_noassreal\n",
    "            lossD += d_loss_noassreal.item()\n",
    "\n",
    "            fake, code = netg(img)\n",
    "            d_out_fake, df1 = netd(fake.detach())\n",
    "            d_loss_fake = nn.ReLU()(1.0 + d_out_fake).mean()\n",
    "\n",
    "            lossD_ += d_loss_fake\n",
    "            lossD += d_loss_fake.item()\n",
    "    #         gradient_penalty = compute_gradient_penalty(netd, ass_label.data, fake.data)\n",
    "            lossD_ = lossD_/3\n",
    "            lossD_.backward()\n",
    "            optimD.step()\n",
    "\n",
    "    #         for p in netd.parameters():\n",
    "    #             p.data.clamp_(-0.01, 0.01)\n",
    "\n",
    "            # update A\n",
    "            lossA = 0\n",
    "            lossA_ = 0\n",
    "            optimA.zero_grad()\n",
    "            assd = th.cat((img, ass_label), 1)\n",
    "            noassd = th.cat((img, noass_label), 1)\n",
    "            faked, code = netg(img)\n",
    "            faked = th.cat((img, faked.detach()), 1)\n",
    "\n",
    "            d_out_assreal,dr1 = neta(assd)\n",
    "            d_loss_assreal = nn.ReLU()(1.0 - d_out_assreal).mean()\n",
    "            lossA += d_loss_assreal.item()\n",
    "            lossA_ += d_loss_assreal\n",
    "\n",
    "            d_out_noassreal,dr2 = neta(noassd)\n",
    "            d_loss_noassreal = nn.ReLU()(1.0 + d_out_noassreal).mean()\n",
    "\n",
    "            lossA_ += d_loss_noassreal\n",
    "            lossA += d_loss_noassreal.item()\n",
    "\n",
    "            d_out_faked, df3 = neta(faked)\n",
    "            d_loss_faked = nn.ReLU()(1.0 + d_out_faked).mean()\n",
    "\n",
    "            lossA_ += d_loss_faked\n",
    "            lossA += d_loss_faked.item()\n",
    "    #         gradient_penalty = compute_gradient_penalty(neta, assd.data, faked.data)\n",
    "            lossA_ = lossA_/3\n",
    "            lossA_.backward()\n",
    "            optimA.step()\n",
    "\n",
    "#         for p in neta.parameters():\n",
    "#             p.data.clamp_(-0.01, 0.01)\n",
    "            \n",
    "        # update G\n",
    "#         if i % n_critic == 0:\n",
    "        lossG = 0\n",
    "        lossG_ = 0\n",
    "        optimG.zero_grad()\n",
    "        fake, A= netg(img)\n",
    "        g_out_fake,_ = netd(fake)\n",
    "        g_loss_fake = - g_out_fake.mean()\n",
    "\n",
    "        lossG += g_loss_fake.item()\n",
    "        lossG_ += g_loss_fake\n",
    "\n",
    "        faked = th.cat((img, fake), 1)\n",
    "        g_out_faked,_ = neta(faked)\n",
    "        g_loss_faked = - g_out_faked.mean()\n",
    "        lossG += g_loss_faked.item()\n",
    "        lossG_ += g_loss_faked\n",
    "        \n",
    "        # constrain on generator\n",
    "        fake_ass, P = netg(ass_img)\n",
    "        fake_noass, N = netg(noass_img)\n",
    "        lossTriplet = F.triplet_margin_loss(A, P, N, margin = margin)\n",
    "        lossG_ += lossTriplet\n",
    "        lossG += lossTriplet.item()\n",
    "#         lossTriplet.backward()\n",
    "        \n",
    "        lossG_ = lossG_/3\n",
    "        lossG_.backward(retain_graph=True)\n",
    "        optimG.step()\n",
    "\n",
    "        if i % 20 == 0:\n",
    "            with th.no_grad():\n",
    "                netg.eval()  #切換\n",
    "                fake,_ = netg(img) \n",
    "                netg.train() #切換回去\n",
    "            fake = (fake + 1) / 2 * 255\n",
    "            real = (ass_label + 1) / 2 * 255\n",
    "            ori = (img + 1) / 2 * 255\n",
    "            al = th.cat((fake, real, ori), 2)\n",
    "            display = make_grid(al, 20).cpu().numpy()\n",
    "            if win1 is None:\n",
    "                win1 = vis.image(display,\n",
    "                                 opts=dict(title=\"train\", caption='train'))\n",
    "            else:\n",
    "                vis.image(display, win=win1)\n",
    "\n",
    "    if epoch % 2==0:   #2   \n",
    "        if win is None:\n",
    "            win = vis.line(X=np.array([[epoch, epoch,\n",
    "                                        epoch]]),\n",
    "                           Y=np.array([[lossG/3, lossA/3, lossD/3]]),\n",
    "                           opts=dict(\n",
    "                               title=Model_Name,\n",
    "                               ylabel='loss',\n",
    "                               xlabel='epochs',\n",
    "                               legend=['lossG', 'lossA', 'lossD']\n",
    "                           ))\n",
    "        else:\n",
    "            vis.line(X=np.array([[epoch, epoch,\n",
    "                                  epoch]]),\n",
    "                     Y=np.array([[lossG/3, lossA/3, lossD/3]]),\n",
    "                     win=win,\n",
    "                     update='append')\n",
    "\n",
    "        with open(Model_dir+\"/snapshot_log.txt\", \"a\") as myfile:\n",
    "            myfile.write('Epoch = {}, ErrG = {}, ErrA = {}, ErrD = {} \\n'.format(\n",
    "            epoch, lossG/3, lossA/3, lossD/3\n",
    "        ))\n",
    "        print('Epoch = {}, ErrG = {}, ErrTri = {}, ErrA = {}, ErrD = {}, Gattn={}, Dattn={}, Aattn={}'.format(\n",
    "            epoch, lossG/3,lossTriplet.item(), lossA/3, lossD/3, netg.attn.gamma.item(), netd.attn.gamma.item(), neta.attn.gamma.item()\n",
    "        ))\n",
    "            \n",
    "    if (epoch>= 300) and epoch%20==0:      \n",
    "        state = {\n",
    "            'netA': neta.state_dict(),\n",
    "            'netG': netg.state_dict(),\n",
    "            'netD': netd.state_dict()\n",
    "        }\n",
    "        th.save(state, Model_dir+'/snapshot'+ Model_Name +'_%d.t7' % epoch)\n",
    "        \n",
    "    if (epoch>= 550) and (lossG/3)<low_loss:  \n",
    "        low_loss = lossG/3\n",
    "        state = {\n",
    "            'netA': neta.state_dict(),\n",
    "            'netG': netg.state_dict(),\n",
    "            'netD': netd.state_dict()\n",
    "        }\n",
    "        th.save(state, Model_dir+'/lowest_snapshot'+ Model_Name +'_%d.t7' % epoch)\n",
    "        with open(Model_dir+\"/snapshot_log.txt\", \"a\") as myfile:\n",
    "            myfile.write('lower_lossG Epoch = {}, ErrG = {}, ErrA = {}, ErrD = {} \\n'.format(\n",
    "            epoch, lossG/3, lossA/3, lossD/3\n",
    "        ))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# dataset loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as th\n",
    "import torch.utils.data as data\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "import random\n",
    "th.cuda.manual_seed(29)\n",
    "th.manual_seed(29)\n",
    "np.random.seed(0)\n",
    "random.seed(0)\n",
    "\n",
    "def loadImage(path):\n",
    "#     print(path)\n",
    "    inImage = cv2.imread(path, 0)\n",
    "    info = np.iinfo(inImage.dtype) #(min = 0 ,max = 255),Maximum value of given dtype.\n",
    "    inImage = inImage.astype(np.float) / info.max # 歸一化\n",
    "#     inImage = inImage.astype(np.float) / 127.5-1 # 歸一化\n",
    "\n",
    "    iw = inImage.shape[1]\n",
    "    ih = inImage.shape[0]\n",
    "    if iw <= ih:\n",
    "        inImage = cv2.resize(inImage, (64, int(64 * ih/iw)))\n",
    "#         print(inImage.shape)\n",
    "    else:\n",
    "        inImage = cv2.resize(inImage, (int(64 * iw / ih), 64)) #(160,80)->(128,64)\n",
    "    inImage = inImage[0:64, 0:64]\n",
    "\n",
    "#     inImage = cv2.resize(inImage, (48, 160))\n",
    "    img = th.from_numpy(2 * inImage - 1).unsqueeze(0) # unsqueeze(0) 在第0維多增加一維 代表灰階，且輸入為-1~1之間\n",
    "    # img shape = [1,64,64]  \n",
    "#     print(th.max(img),th.min(img),th.mean(img))\n",
    "    return img\n",
    "\n",
    "\n",
    "class CASIABDataset(data.Dataset):\n",
    "    def __init__(self, data_dir, target):\n",
    "        self.data_dir = data_dir\n",
    "        self.ids = np.arange(1, 63) #1-62\n",
    "        self.cond = ['bg-01', 'bg-02', 'cl-01', 'cl-02',\n",
    "                     'nm-01', 'nm-02', 'nm-03', 'nm-04',\n",
    "                     'nm-05', 'nm-06']\n",
    "#         self.angles = ['000', '018', '036', '054', '072',\n",
    "#                        '108', '126', '144', '162', '180']  # originally\n",
    "        self.angles = ['000', '018', '036', '054', '072', '090',\n",
    "                       '108', '126', '144', '162', '180']\n",
    "        self.n_id = 62\n",
    "        self.n_cond = len(self.cond)\n",
    "        self.n_ang = len(self.angles)\n",
    "        print(\"n_con=\",self.n_cond,',n_ang=',self.n_ang)\n",
    "        self.target = target\n",
    "        print('target = ',self.target)\n",
    "        \n",
    "\n",
    "    def __getitem__(self, index):\n",
    "            # r1 is GT target\n",
    "            # r2 is irrelevant GT target\n",
    "            # r3 is source image   \n",
    "        while(True):           \n",
    "            id1 = th.randint(0, self. n_id, (1,)).item() + 1\n",
    "            id1 = '%03d' % id1\n",
    "            cond1 = th.randint(4, self.n_cond, (1,)).item()\n",
    "            cond1 = self.cond[int(cond1)]\n",
    "            r1 = id1 + '/' + cond1 + '/' + id1 + '-' + cond1 + '-' + self.target+'.png'                     \n",
    "            if os.path.exists(self.data_dir + r1):\n",
    "                break\n",
    "#         print('r1=,',r1)\n",
    "\n",
    "\n",
    "        id2 = id1 \n",
    "        while(True):\n",
    "            id2 = th.randint(0, self. n_id, (1,)).item() + 1\n",
    "            id2 = '%03d' % id2\n",
    "            cond2 = th.randint(4, self.n_cond, (1,)).item()\n",
    "            cond2 = int(cond2)\n",
    "            cond2 = self.cond[cond2]\n",
    "            r2 = id2 + '/' + cond2 + '/' +  id2 + '-' + cond2 + '-' + self.target+'.png'\n",
    "            \n",
    "            cond2_1 = th.randint(4, self.n_cond, (1,)).item() # not all conditions\n",
    "            cond2_1 = self.cond[int(cond2_1)]\n",
    "            angle2_1 = th.randint(0, self.n_ang, (1,)).item()\n",
    "            angle2_1 = self.angles[int(angle2_1)]\n",
    "            r2_1 = id2 + '/' + cond2_1 + '/' + id2 + '-' + cond2_1 + '-' + angle2_1+'.png'\n",
    "            if os.path.exists(self.data_dir + r2) and (id2!=id1) and os.path.exists(self.data_dir + r2_1) and (r2!=r2_1):\n",
    "                break\n",
    "#         print('r2=,',r2)\n",
    "#         print('r2_1=,',r2_1)\n",
    "\n",
    "        while True:\n",
    "            angle = th.randint(0, self.n_ang, (1,)).item()\n",
    "            angle = int(angle)\n",
    "            angle = self.angles[angle]\n",
    "            cond3 = th.randint(0, self.n_cond, (1,)).item()\n",
    "            cond3 = int(cond3)\n",
    "            cond3 = self.cond[cond3]\n",
    "            r3 = id1 + '/' + cond3 + '/'  +  id1 + '-' + cond3 + '-' + angle + '.png'\n",
    "            \n",
    "            cond3_1 = th.randint(4, self.n_cond, (1,)).item() #not all con\n",
    "            cond3_1 = self.cond[int(cond3_1)]\n",
    "            angle3_1 = th.randint(0, self.n_ang, (1,)).item()\n",
    "            angle3_1 = self.angles[int(angle3_1)]\n",
    "            r3_1 = id1 + '/' + cond3_1 + '/' + id1 + '-' + cond3_1 + '-' + angle3_1+'.png'\n",
    "            if os.path.exists(self.data_dir + r3) and os.path.exists(self.data_dir + r3_1) and (r3!=r3_1):\n",
    "                break\n",
    "#         print('r3=,',r3)\n",
    "#         print('r3_1=,',r3_1,'\\n')\n",
    "\n",
    "        img1 = loadImage(self.data_dir + r1)\n",
    "        img2 = loadImage(self.data_dir + r2)\n",
    "        img2_1 = loadImage(self.data_dir + r2_1)\n",
    "        img3 = loadImage(self.data_dir + r3)\n",
    "        img3_1 = loadImage(self.data_dir + r3_1)\n",
    "        return img1, img2, img2_1, img3, img3_1\n",
    "    \n",
    "    def __len__(self):\n",
    "        total_len = 0\n",
    "        total_len = len(glob.glob(self.data_dir))\n",
    "        return 6400"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py36_ting_cv]",
   "language": "python",
   "name": "conda-env-py36_ting_cv-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
